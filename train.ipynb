{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gym_environment\n",
    "env = gym_environment.Environment()\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HIDDEN_LAYER_SIZE = 128\n",
    "class DQN(nn.Module):\n",
    "\tdef __init__(self, observation_size, action_size):\n",
    "\t\tsuper().__init__()\n",
    "\t\t# self.fc1 = nn.Linear(observation_size, HIDDEN_LAYER_SIZE)\n",
    "\t\t# self.fc2 = nn.Linear(HIDDEN_LAYER_SIZE, HIDDEN_LAYER_SIZE)\n",
    "\t\t# self.fc3 = nn.Linear(HIDDEN_LAYER_SIZE, action_size)\n",
    "\t\tself.fc1 = nn.Linear(observation_size, 32)\n",
    "\t\tself.fc2 = nn.Linear(32, 64)\n",
    "\t\tself.fc3 = nn.Linear(64, 128)\n",
    "\t\tself.fc4 = nn.Linear(128, action_size)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tx = F.relu(self.fc1(x))\n",
    "\t\tx = F.relu(self.fc2(x))\n",
    "\t\tx = F.relu(self.fc3(x))\n",
    "\t\treturn self.fc4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "EPS_START = 0.9\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4\n",
    "\n",
    "action_size = env.action_space.n\n",
    "state, _ = env.reset()\n",
    "observation_size = len(state)\n",
    "\n",
    "policy_net = DQN(observation_size, action_size).to(device)\n",
    "target_net = DQN(observation_size, action_size).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "\tglobal steps_done\n",
    "\tsample = random.random()\n",
    "\teps_threshold = EPS_END + (EPS_START - EPS_END) * \\\n",
    "\t\tmath.exp(-1. * steps_done / EPS_DECAY)\n",
    "\tsteps_done += 1\n",
    "\tif sample > eps_threshold:\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\t# t.max(1) will return the largest column value of each row.\n",
    "\t\t\t# second column on max result is index of where max element was\n",
    "\t\t\t# found, so we pick action with the larger expected reward.\n",
    "\t\t\treturn policy_net(state).max(1).indices.view(1, 1)\n",
    "\telse:\n",
    "\t\treturn torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 1 sum -3199.5339712955756                                                                                                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 108\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m policy_net_state_dict:\n\u001b[1;32m    107\u001b[0m \ttarget_net_state_dict[key] \u001b[38;5;241m=\u001b[39m policy_net_state_dict[key]\u001b[38;5;241m*\u001b[39mTAU \u001b[38;5;241m+\u001b[39m target_net_state_dict[key]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mTAU)\n\u001b[0;32m--> 108\u001b[0m \u001b[43mtarget_net\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_net_state_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m rewards\u001b[38;5;241m.\u001b[39mappend(reward\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/Desktop/Programs/thing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2175\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2168\u001b[0m         out \u001b[38;5;241m=\u001b[39m hook(module, incompatible_keys)\n\u001b[1;32m   2169\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, (\n\u001b[1;32m   2170\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHooks registered with ``register_load_state_dict_post_hook`` are not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2171\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected to return new values, if incompatible_keys need to be modified,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2172\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit should be done inplace.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2173\u001b[0m         )\n\u001b[0;32m-> 2175\u001b[0m \u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2176\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m load\n\u001b[1;32m   2178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m strict:\n",
      "File \u001b[0;32m~/Desktop/Programs/thing/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:2162\u001b[0m, in \u001b[0;36mModule.load_state_dict.<locals>.load\u001b[0;34m(module, local_state_dict, prefix)\u001b[0m\n\u001b[1;32m   2160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m child \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2161\u001b[0m         child_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m-> 2162\u001b[0m         child_state_dict \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m local_state_dict\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstartswith\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild_prefix\u001b[49m\u001b[43m)\u001b[49m}\n\u001b[1;32m   2163\u001b[0m         load(child, child_state_dict, child_prefix)  \u001b[38;5;66;03m# noqa: F821\u001b[39;00m\n\u001b[1;32m   2165\u001b[0m \u001b[38;5;66;03m# Note that the hook can modify missing_keys and unexpected_keys.\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import const\n",
    "\n",
    "def my_optimize_model():\n",
    "\tif len(memory) < BATCH_SIZE:\n",
    "\t\treturn\n",
    "\t\n",
    "\ttransitions = memory.sample(BATCH_SIZE)\n",
    "\n",
    "\n",
    "def optimize_model():\n",
    "\tif len(memory) < BATCH_SIZE:\n",
    "\t\treturn\n",
    "\t\n",
    "\ttransitions = memory.sample(BATCH_SIZE)\n",
    "\tbatch = Transition(*zip(*transitions))\n",
    "\t# print(batch)\n",
    "\n",
    "\tnon_final_mask = torch.tensor(list(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
    "\tnon_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "\tstate_batch = torch.cat(batch.state)\n",
    "\taction_batch = torch.cat(batch.action)\n",
    "\treward_batch = torch.cat(batch.reward)\n",
    "\n",
    "\t# Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "\t# columns of actions taken. These are the actions which would've been taken\n",
    "\t# for each batch state according to policy_net\n",
    "\tstate_action_values = policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "\t# Compute V(s_{t+1}) for all next states.\n",
    "\t# Expected values of actions for non_final_next_states are computed based\n",
    "\t# on the \"older\" target_net; selecting their best reward with max(1).values\n",
    "\t# This is merged based on the mask, such that we'll have either the expected\n",
    "\t# state value or 0 in case the state was final.\n",
    "\tnext_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "\twith torch.no_grad():\n",
    "\t\tnext_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
    "\t# Compute the expected Q values\n",
    "\texpected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "\t# Compute Huber loss\n",
    "\tcriterion = nn.SmoothL1Loss()\n",
    "\tloss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "\t# Optimize the model\n",
    "\toptimizer.zero_grad()\n",
    "\tloss.backward()\n",
    "\t# In-place gradient clipping\n",
    "\ttorch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "\toptimizer.step()\n",
    "\n",
    "\n",
    "if torch.cuda.is_available() or torch.backends.mps.is_available():\n",
    "\tnum_episodes = 600\n",
    "else:\n",
    "\tnum_episodes = 600\n",
    "\n",
    "# num_episodes = 1\n",
    "\n",
    "xvalues = np.arange(1441)\n",
    "temps = np.zeros(1441)\n",
    "target = np.zeros(1441)\n",
    "reward2 = np.zeros(1441)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "\trewards = []\n",
    "\ttotal_reward = 0\n",
    "\t# Initialize the environment and get its state\n",
    "\n",
    "\tweather_start = random.randrange(0, len(const.OUTSIDE_TEMP) - 1440)\n",
    "\tstate, info = env.reset(num_setpoints=random.randint(2, 7), start_time=weather_start)\n",
    "\t\n",
    "\tstate = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "\tfor t in count():\n",
    "\t\taction = select_action(state)\n",
    "\t\tobservation, reward, terminated = env.step(action.item())\n",
    "\n",
    "\t\t# temps[t] = env._cur_temp\n",
    "\t\t# target[t] = env._target\n",
    "\t\t# reward2[t] = reward\n",
    "\n",
    "\t\treward = torch.tensor([reward], device=device)\n",
    "\t\tdone = terminated \n",
    "\n",
    "\t\tif terminated:\n",
    "\t\t\tnext_state = None\n",
    "\t\telse:\n",
    "\t\t\tnext_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "\t\t# Store the transition in memory\n",
    "\t\tmemory.push(state, action, next_state, reward)\n",
    "\n",
    "\t\t# Move to the next state\n",
    "\t\tstate = next_state\n",
    "\n",
    "\t\t# Perform one step of the optimization (on the policy network)\n",
    "\t\toptimize_model()\n",
    "\n",
    "\t\t# Soft update of the target network's weights\n",
    "\t\t# θ′ ← τ θ + (1 −τ )θ′\n",
    "\t\ttarget_net_state_dict = target_net.state_dict()\n",
    "\t\tpolicy_net_state_dict = policy_net.state_dict()\n",
    "\t\tfor key in policy_net_state_dict:\n",
    "\t\t\ttarget_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "\t\ttarget_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "\t\trewards.append(reward.item())\n",
    "\t\tif done:\n",
    "\t\t\tprint(f\"{' ' * 200}\\repisode {i_episode} sum {sum(rewards)}\", end=\"\\r\")\n",
    "\t\t\tbreak\n",
    "\n",
    "# plt.plot(xvalues, temps, linewidth=0.1)\n",
    "# plt.plot(xvalues, target, linewidth=0.1)\n",
    "# plt.plot(xvalues, reward2, linewidth=0.1)\n",
    "# plt.ioff()\n",
    "# plt.savefig(\"out.png\", dpi=3000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy_net.state_dict(), \"combination_3k_2.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net.load_state_dict(torch.load('subtract_4_3k.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import const\n",
    "import random\n",
    "import time\n",
    "import agents.dumb_agent\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_xlabel(\"time (min)\")\n",
    "# ax1.set_ylim(10, 30)\n",
    "# ax1.set_yticks(np.arange(10, 31))\n",
    "ax1.set_ylabel(\"deg C\")\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylim(-1, 5)\n",
    "ax2.set_ylabel(\"mean temp deviation or ac/heater power\")\n",
    "\n",
    "sim_max = 2880\n",
    "\n",
    "# weather_start = 695991\n",
    "weather_start = random.randrange(0, len(const.OUTSIDE_TEMP) - sim_max)\n",
    "\n",
    "xvalues = np.arange(0, sim_max)\n",
    "temperatures = np.zeros(sim_max)\n",
    "setpoints = np.zeros(sim_max)\n",
    "outside_temp = np.zeros(sim_max)\n",
    "on_off = np.zeros(sim_max)\n",
    "mean_dev = np.zeros(sim_max)\n",
    "old_action = 0\n",
    "cycles = 0\n",
    "\n",
    "deviation_sum = 0\n",
    "\n",
    "state, _ = env.reset(num_setpoints=5, length=sim_max, start_time=weather_start)\n",
    "\n",
    "for i in range(sim_max):\t\n",
    "\tpower = policy_net(torch.tensor([state])).max(1).indices.view(1, 1).item()\n",
    "\t# power = agents.dumb_agent.agent(env.get_cur_temp(), const.OUTSIDE_TEMP[weather_start + i], env.get_setpoint(), [-1, -0.75, -0.5, -0.25, 0, 1][old_action])\n",
    "\t# power = [-1, -0.75, -0.5, -0.25, 0, 1].index(power)\n",
    "\tif env._sgn(power) != env._sgn(old_action):\n",
    "\t\tcycles += 1\n",
    "\told_action = power\n",
    "\n",
    "\ttemperatures[i] = env.get_cur_temp()\n",
    "\tsetpoints[i] = env.get_setpoint()\n",
    "\toutside_temp[i] = const.OUTSIDE_TEMP[weather_start + i]\n",
    "\n",
    "\ton_off[i] = env._actions[power]\n",
    "\n",
    "\tdeviation_sum += abs(temperatures[i] - env.get_setpoint())\n",
    "\tstate, reward, _ = env.step(power)\n",
    "\n",
    "\tmean_dev[i] = deviation_sum / (i + 1)\n",
    "\n",
    "ax1.plot(xvalues, temperatures, color=\"red\", linewidth=0.1)\n",
    "ax1.plot(xvalues, setpoints, color=\"blue\", linewidth=0.1)\n",
    "ax1.plot(xvalues, outside_temp, color=\"green\", linewidth=0.1)\n",
    "ax2.plot(xvalues, on_off, color=\"black\", linewidth=0.1)\n",
    "ax2.plot(xvalues, mean_dev, color=\"purple\", linewidth=0.1)\n",
    "# plt.show()\n",
    "plt.savefig(\"old2.png\", dpi=1000)\n",
    "\n",
    "cycles\n",
    "# m2K/W * m2 * K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import agents.dumb_agent\n",
    "import const\n",
    "import random\n",
    "import time\n",
    "import agents.pid_agent\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "ax1.set_ylabel(\"deg C\")\n",
    "\n",
    "# ax2 = ax1.twinx()\n",
    "# ax2.set_ylabel(\"cycles\")\n",
    "\n",
    "episode_count = 100\n",
    "sim_max = 2880\n",
    "num_setpoints = 5\n",
    "\n",
    "xvalues = np.arange(episode_count)\n",
    "stupid_dev = np.zeros(episode_count)\n",
    "stupid_cycles = np.zeros(episode_count)\n",
    "dqn_dev = np.zeros(episode_count)\n",
    "dqn_cycles = np.zeros(episode_count)\n",
    "\n",
    "for i in range(episode_count):\n",
    "\tdeviation_sum = 0\n",
    "\t# weather_start = 0\n",
    "\tweather_start = random.randrange(0, len(const.OUTSIDE_TEMP) - sim_max)\n",
    "\tstate, _ = env.reset(num_setpoints=num_setpoints, length=sim_max, start_time=weather_start)\n",
    "\told_action = 0\n",
    "\tcycles = 0\n",
    "\tfor t in range(sim_max):\t\n",
    "\t\tpower = policy_net(torch.tensor([state])).max(1).indices.view(1, 1).item()\n",
    "\t\tif env._sgn(power) != env._sgn(old_action):\n",
    "\t\t\tcycles += 1\n",
    "\t\told_action = power\n",
    "\n",
    "\t\tdeviation_sum += abs(env.get_cur_temp() - env.get_setpoint())\n",
    "\t\tstate, reward, _ = env.step(power)\n",
    "\tdqn_dev[i] = deviation_sum / sim_max\n",
    "\tdqn_cycles[i] = cycles\n",
    "\tprint(f\"{i + 1}/{episode_count}\", end=\"\\r\")\n",
    "\n",
    "print(\"             \", end=\"\\r\")\n",
    "for i in range(episode_count):\n",
    "\tdeviation_sum = 0\n",
    "\t# weather_start = 0\n",
    "\tweather_start = random.randrange(0, len(const.OUTSIDE_TEMP) - sim_max)\n",
    "\tstate, _ = env.reset(num_setpoints=num_setpoints, length=sim_max, start_time=weather_start)\n",
    "\told_action = 0\n",
    "\tcycles = 0\n",
    "\tfor t in range(sim_max):\t\n",
    "\t\tpower = agents.dumb_agent.agent(env.get_cur_temp(), const.OUTSIDE_TEMP[weather_start + t], env.get_setpoint(), env._actions[old_action])\n",
    "\t\tpower = env._actions.index(power)\n",
    "\t\tif env._sgn(power) != env._sgn(old_action):\n",
    "\t\t\tcycles += 1\n",
    "\t\told_action = power\n",
    "\n",
    "\t\tdeviation_sum += abs(env.get_cur_temp() - env.get_setpoint())\n",
    "\t\tstate, reward, _ = env.step(power)\n",
    "\tstupid_dev[i] = deviation_sum / sim_max\n",
    "\tif deviation_sum / sim_max > 11:\n",
    "\t\tprint(\"VERY BAD\", weather_start)\n",
    "\tstupid_cycles[i] = cycles\n",
    "\tprint(f\"{i + 1}/{episode_count}\", end=\"\\r\")\n",
    "\n",
    "ax1.plot(xvalues, dqn_dev, linewidth=0.5, color=\"blue\")\n",
    "# ax2.plot(xvalues, dqn_cycles, linewidth=0.5, color=\"purple\")\n",
    "ax1.plot(xvalues, stupid_dev, linewidth=0.5, color=\"orange\")\n",
    "# ax2.plot(xvalues, stupid_cycles, linewidth=0.5, color=\"red\")\n",
    "plt.savefig(\"out.png\", dpi=1000)\n",
    "\n",
    "print(f\"dqn dev mean {np.mean(dqn_dev)}\")\n",
    "print(f\"dqn dev median {np.median(dqn_dev)}\")\n",
    "print(f\"dqn dev min {np.min(dqn_dev)}\")\n",
    "print(f\"dqn dev max {np.max(dqn_dev)}\")\n",
    "\n",
    "print(f\"dqn cycle mean {np.mean(dqn_cycles)}\")\n",
    "print(f\"dqn cycle median {np.median(dqn_cycles)}\")\n",
    "print(f\"dqn cycle min {np.min(dqn_cycles)}\")\n",
    "print(f\"dqn cycle max {np.max(dqn_cycles)}\")\n",
    "\n",
    "print(f\"stupid dev mean {np.mean(stupid_dev)}\")\n",
    "print(f\"stupid dev median {np.median(stupid_dev)}\")\n",
    "print(f\"stupid dev min {np.min(stupid_dev)}\")\n",
    "print(f\"stupid dev max {np.max(stupid_dev)}\")\n",
    "\n",
    "print(f\"stupid cycle mean {np.mean(stupid_cycles)}\")\n",
    "print(f\"stupid cycle median {np.median(stupid_cycles)}\")\n",
    "print(f\"stupid cycle min {np.min(stupid_cycles)}\")\n",
    "print(f\"stupid cycle max {np.max(stupid_cycles)}\")\n",
    "\n",
    "# m2K/W * m2 * K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in policy_net.parameters() if p.requires_grad)\n",
    "pytorch_total_params"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
