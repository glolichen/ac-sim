# AI Thermostat

This is the repository for a Reinforcement Learning-powered HVAC controller made under Professor Michael Ferdman at CSIRE 2024.

## House Configuration

You may configure your own house to run tests on with any number of rooms. There are currently 5 such houses created. Please refer to houseconfig.md for documentation on the format of those files.

## Weather Files

Currently, there are 2 CSV weather files to get around GitHub's 1,000,000 line limit. Currently they are only used for outside temperature and are loaded in const.py.

## Gen Data

These will generate a list of box plots with data produced by the selected model after each run. In `gen_data_dagger`, `-m dit/to/model.zip` will run the model `x.zip` that is generated by `train_from_dagger` (which applies PPO to a trained DAgger model) and `dagger_learn` (which imitates the behavior of the "expert" `DumbPolicy`). For both `gen_data`s, use `-o dir/to/output.png` to tell it where to save the produced image. You are required to enter the number of episodes (days) to generate as a command line argument. **Make sure that if you define your custom policy you DO NOT specify net_arch**, see warnings below.

## Sim / Time Series

These will generate a time series of the state of the house. The bottom box represents if the setting of the AC (-1 = max cool, 0 = off, 1 = max heat). Each box represents a room. The green line is the same in each plot and represents outside temperature. Red line is current room temperature and blue line is the setpoint (target). Command line arguments mean the same as in Gen Data. A new command line argument can be passed, `-t [integer]` which tells the program how many minutes to simulate. Default is 1440 or 1 day.

## Dumb Agents

Hardcoded agents that we are trying (unsuccessfully so far) to beat. To change which agent to use when using Sim or Gen Data locate the `from agents/file import agent` and change `file` to the agent you want.

## Deep Q Network
The `main` branch is primarily for imitation/DAgger based methods. The simple Deep Q Network based system is located on the `deep-q-network` branch.

## Install

Known to work on Linux with Python 3.8.*. Python 3.12 may not work because of stable baselines 3 issues. Install dependencies:
```
pip install 'imitation[all]' 'stable-baselines3[extra]'
```

## Warnings before use

Below is a list of known points of strange or buggy behavior. Please note that there are probably plenty more than I am not aware of or forgot.
 * `housebuilder` is not fully tested. You should check that the returned House object makes sense. The file parsing process may also throw `jayden li programming error`s which are safeguards against unexpected behavior and are largely caused by programming mistakes.
 * Before running `gen_data*` or `sim*` you should change a `num_rooms` variable. This tells the program how many rooms are in the house.
 * There is a bug in `imitation` that makes it impossible to load saved policies. See bug report filed [here](https://github.com/HumanCompatibleAI/imitation/issues/857). A very janky solution is detailed there.
 * There are 2 gym environmnents: `gym_environmnent.Environment` and `gym_environment2.Environment`. The former uses linear deviation penalty and only applies penalty for number of toggles after a certain point. The latter uses a 6th degree function which will make penalty worse based on how bad the deviation is and applies a constant penalty for each toggle. Note the difference in observation size between these two environments, and that when training dagger with `dagger_learn`, you will have to change the DumbPolicy to use the new observation shape. You will have to manually tune these functions to find a reasonable balance between comfort and cycle count.

Have fun!
