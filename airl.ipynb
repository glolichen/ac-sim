{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayden/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gym_environment\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms.adversarial.airl import AIRL\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.policies.serialize import load_policy\n",
    "from imitation.rewards.reward_nets import BasicShapedRewardNet\n",
    "from imitation.util.networks import RunningNorm\n",
    "from imitation.util.util import make_vec_env\n",
    "import imitation.policies.base\n",
    "\n",
    "import enum\n",
    "from typing import Union, Dict\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "gym.register(\n",
    "\tid=\"HVAC-v0\",\n",
    "\tentry_point=gym_environment.Environment,\n",
    "\tmax_episode_steps=1440,\n",
    ")\n",
    "env = make_vec_env(\n",
    "    \"HVAC-v0\",\n",
    "    rng=np.random.default_rng(SEED),\n",
    "    n_envs=8,\n",
    "    post_wrappers=[lambda env, _: RolloutInfoWrapper(env)],  # to compute rollouts\n",
    ")\n",
    "\n",
    "class DumbPolicy(imitation.policies.base.NonTrainablePolicy):\n",
    "\tclass Status(enum.Enum):\n",
    "\t\tNEED_COOL = 0\n",
    "\t\tWANT_COOL = 1\n",
    "\t\tNEED_HEAT = 2\n",
    "\t\tWANT_HEAT = 3\n",
    "\t\tEQUAL = 4\n",
    "\tdef _choose_action(self, obs: Union[np.ndarray, Dict[str, np.ndarray]],) -> int:\n",
    "\t\tnum_rooms = int((len(obs) - 3) / 4)\n",
    "\n",
    "\t\tepsilon = 0.9\n",
    "\t\tstatuses = []\n",
    "\t\tneed_heat, need_cool = 0, 0\n",
    "\t\tbadness_heat, badness_cool = 0, 0\n",
    "\t\tmin_temp, max_temp = 100000, -100000\n",
    "\n",
    "\t\told_ac_status = obs[num_rooms * 2 + 2]\n",
    "\t\told_dampers = [[]]\n",
    "\t\tfor i in range(num_rooms):\n",
    "\t\t\told_dampers[0].append(bool(obs[num_rooms * 2 + 4 + i * 2]))\n",
    "\n",
    "\t\tfor i in range(num_rooms):\n",
    "\t\t\ttemp, setp = obs[i * 2], obs[i * 2 + 1]\n",
    "\t\t\tmin_temp = min(min_temp, temp)\n",
    "\t\t\tmax_temp = max(max_temp, temp)\n",
    "\t\t\tif temp < setp - epsilon:\n",
    "\t\t\t\tstatuses.append(self.Status.NEED_HEAT.value)\n",
    "\t\t\t\tbadness_heat += abs(temp - setp)\n",
    "\t\t\t\tneed_heat += 1\n",
    "\t\t\telif temp < setp:\n",
    "\t\t\t\tstatuses.append(self.Status.WANT_HEAT.value)\n",
    "\t\t\t\tbadness_heat += abs(temp - setp)\n",
    "\t\t\telif temp > setp + epsilon:\n",
    "\t\t\t\tstatuses.append(self.Status.NEED_COOL.value)\n",
    "\t\t\t\tbadness_cool += abs(temp - setp)\n",
    "\t\t\t\tneed_cool += 1\n",
    "\t\t\telif temp > setp:\n",
    "\t\t\t\tstatuses.append(self.Status.WANT_COOL.value)\n",
    "\t\t\t\tbadness_cool += abs(temp - setp)\n",
    "\t\t\telse:\n",
    "\t\t\t\tstatuses.append(self.Status.EQUAL.value)\n",
    "\n",
    "\t\toutside_temp = obs[num_rooms * 2]\n",
    "\t\tdampers = [[]]\n",
    "\t\tif need_heat > need_cool:\n",
    "\t\t\tif max_temp >= outside_temp:\n",
    "\t\t\t\tfor status in statuses:\n",
    "\t\t\t\t\tif status == self.Status.NEED_HEAT.value or status == self.Status.WANT_HEAT.value:\n",
    "\t\t\t\t\t\tdampers[0].append(False)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tdampers[0].append(True)\n",
    "\t\t\t\treturn env.get_attr(\"actions\")[0].index((1, dampers))\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn env.get_attr(\"actions\")[0].index((0, old_dampers))\n",
    "\t\tif need_cool > need_heat:\n",
    "\t\t\tif min_temp <= outside_temp:\n",
    "\t\t\t\tfor status in statuses:\n",
    "\t\t\t\t\tif status == self.Status.NEED_COOL.value or status == self.Status.WANT_COOL.value:\n",
    "\t\t\t\t\t\tdampers[0].append(False)\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tdampers[0].append(True)\n",
    "\t\t\t\treturn env.get_attr(\"actions\")[0].index((-1, dampers))\n",
    "\t\t\telse:\n",
    "\t\t\t\treturn env.get_attr(\"actions\")[0].index((0, old_dampers))\n",
    "\n",
    "\t\tif need_cool > 0 and need_heat > 0:\n",
    "\t\t\tif badness_cool > badness_heat:\n",
    "\t\t\t\tif min_temp <= outside_temp:\n",
    "\t\t\t\t\tfor status in statuses:\n",
    "\t\t\t\t\t\tif status == self.Status.NEED_COOL.value or status == self.Status.WANT_COOL.value:\n",
    "\t\t\t\t\t\t\tdampers[0].append(False)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tdampers[0].append(True)\n",
    "\t\t\t\t\treturn env.get_attr(\"actions\")[0].index((-1, dampers))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn env.get_attr(\"actions\")[0].index((0, old_dampers))\n",
    "\t\t\tif badness_heat > badness_cool:\n",
    "\t\t\t\tif max_temp >= outside_temp:\n",
    "\t\t\t\t\tfor status in statuses:\n",
    "\t\t\t\t\t\tif status == self.Status.NEED_HEAT.value or status == self.Status.WANT_HEAT.value:\n",
    "\t\t\t\t\t\t\tdampers[0].append(False)\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tdampers[0].append(True)\n",
    "\t\t\t\t\treturn env.get_attr(\"actions\")[0].index((1, dampers))\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\treturn env.get_attr(\"actions\")[0].index((0, old_dampers))\n",
    "\t\t\t\n",
    "\t\treturn env.get_attr(\"actions\")[0].index((old_ac_status, old_dampers))\n",
    "expert = DumbPolicy(env.observation_space, env.action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayden/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.actions to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.actions` for environment variables or `env.get_wrapper_attr('actions')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "rollouts = rollout.rollout(\n",
    "\texpert,\n",
    "\tenv,\n",
    "\trollout.make_sample_until(min_episodes=3000),\n",
    "\trng=np.random.default_rng(SEED),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"rollouts.pkl\", \"wb\") as out:\n",
    "\tpickle.dump(rollouts, out, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rollouts = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rollouts.pkl\", \"rb\") as file:\n",
    "\trollouts = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m\n\u001b[1;32m     18\u001b[0m airl_trainer \u001b[38;5;241m=\u001b[39m AIRL(\n\u001b[1;32m     19\u001b[0m     demonstrations\u001b[38;5;241m=\u001b[39mrollouts,\n\u001b[1;32m     20\u001b[0m     demo_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     25\u001b[0m     reward_net\u001b[38;5;241m=\u001b[39mreward_net,\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     28\u001b[0m env\u001b[38;5;241m.\u001b[39mseed(SEED)\n\u001b[0;32m---> 29\u001b[0m learner_rewards_before_training, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearner\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m airl_trainer\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m20000\u001b[39m)  \u001b[38;5;66;03m# Train for 2_000_000 steps to match expert.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m env\u001b[38;5;241m.\u001b[39mseed(SEED)\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:88\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     86\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m---> 88\u001b[0m     actions, states \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     90\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisode_starts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n\u001b[1;32m     95\u001b[0m     current_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m rewards\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/stable_baselines3/common/base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[1;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/stable_baselines3/common/policies.py:368\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 368\u001b[0m     actions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert to numpy, and reshape to the original action shape\u001b[39;00m\n\u001b[1;32m    370\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape))  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/stable_baselines3/common/policies.py:717\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_predict\u001b[39m(\u001b[38;5;28mself\u001b[39m, observation: PyTorchObs, deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    710\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;124;03m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/stable_baselines3/common/distributions.py:88\u001b[0m, in \u001b[0;36mDistribution.get_actions\u001b[0;34m(self, deterministic)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;124;03mReturn actions according to the probability distribution.\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m:param deterministic:\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deterministic:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample()\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/stable_baselines3/common/distributions.py:301\u001b[0m, in \u001b[0;36mCategoricalDistribution.mode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmode\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m th\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m th\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobs\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/torch/distributions/utils.py:148\u001b[0m, in \u001b[0;36mlazy_property.__get__\u001b[0;34m(self, instance, obj_type)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _lazy_property_and_property(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39menable_grad():\n\u001b[0;32m--> 148\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28msetattr\u001b[39m(instance, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapped\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, value)\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/torch/distributions/categorical.py:101\u001b[0m, in \u001b[0;36mCategorical.probs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;129m@lazy_property\u001b[39m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprobs\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlogits_to_probs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/torch/distributions/utils.py:90\u001b[0m, in \u001b[0;36mlogits_to_probs\u001b[0;34m(logits, is_binary)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_binary:\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(logits)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Programs/aithermostat/venv/lib/python3.8/site-packages/torch/nn/functional.py:1888\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1886\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[1;32m   1887\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1888\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1890\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learner = PPO(\n",
    "    env=env,\n",
    "    policy=MlpPolicy,\n",
    "    batch_size=64,\n",
    "    ent_coef=0.0,\n",
    "    learning_rate=0.0005,\n",
    "    gamma=0.95,\n",
    "    clip_range=0.1,\n",
    "    vf_coef=0.1,\n",
    "    n_epochs=5,\n",
    "    seed=SEED,\n",
    ")\n",
    "reward_net = BasicShapedRewardNet(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    normalize_input_layer=RunningNorm,\n",
    ")\n",
    "airl_trainer = AIRL(\n",
    "    demonstrations=rollouts,\n",
    "    demo_batch_size=2048,\n",
    "    gen_replay_buffer_capacity=512,\n",
    "    n_disc_updates_per_round=16,\n",
    "    venv=env,\n",
    "    gen_algo=learner,\n",
    "    reward_net=reward_net,\n",
    ")\n",
    "\n",
    "env.seed(SEED)\n",
    "learner_rewards_before_training, _ = evaluate_policy(\n",
    "    learner, env, 100, return_episode_rewards=True,\n",
    ")\n",
    "airl_trainer.train(20000)  # Train for 2_000_000 steps to match expert.\n",
    "env.seed(SEED)\n",
    "learner_rewards_after_training, _ = evaluate_policy(\n",
    "    learner, env, 100, return_episode_rewards=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save(reward_net.potential.state_dict(), \"reward_net.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
